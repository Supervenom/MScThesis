
@inproceedings{lamping_algorithm_1990,
	title = {An Algorithm for Optimal Lambda Calculus Reduction},
	abstract = {We present an algorithm for lambda expression reduction that avoids any copying that could later cause duplication of work. It is optimal in the sense defined by Lévy. The basis of the algorithm is a graphical representation of the kinds of commonality that can arise from substitutions; the idea can be adapted to represent other kinds of expressions besides lambda expressions. The algorithm is also well suited to parallel implementations, consisting of a fixed set of local graph rewrite rules.},
	pages = {16--30},
	booktitle = {Proceedings of the 17th {POPL}},
	publisher = {{ACM}},
	author = {Lamping, John},
	date = {1990}
}

@thesis{levy_reductions_1978,
	title = {Réductions correctes et optimales dans le lambda-calcul},
	institution = {Université Paris 7},
	type = {phdthesis},
	author = {Lévy, Jean-Jacques},
	date = {1978}
}

@article{asperti_parallel_2001,
	title = {Parallel Beta Reduction Is Not Elementary Recursive},
	volume = {170},
	abstract = {We analyze the inherent complexity of implementing Lévy's notion of optimal evaluation for the λ-calculus, where similar redexes are contracted in one step via so-called parallel β-reduction. Optimal evaluation was finally realized by Lamping, who introduced a beautiful graph reduction technology for sharing evaluation contexts dual to the sharing of values. His pioneering insights have been modified and improved in subsequent implementations of optimal reduction. We prove that the cost of parallel β-reduction is not bounded by any Kalmár-elementary recursive function. Not only do we establish that the parallel β-step cannot be a unit-cost operation, we demonstrate that the time complexity of implementing a sequence of n parallel β-steps is not bounded as O(2n), O(22n), O(222n), or in general, O(Kl(n)), where Kl(n) is a fixed stack of l 2's with an n on top. A key insight, essential to the establishment of this non-elementary lower bound, is that any simply typed λ-term can be reduced to normal form in a number of parallel β-steps that is only polynomial in the length of the explicitly typed term. The result follows from Statman's theorem that deciding equivalence of typed λ-terms is not elementary recursive. The main theorem gives a lower bound on the work that must be done by any technology that implements Lévy's notion of optimal reduction. However, in the significant case of Lamping's solution, we make some important remarks addressing how work done by β-reduction is translated into equivalent work carried out by his bookkeeping nodes. In particular, we identify the computational paradigms of superposition of values and of higher-order sharing, appealing to compelling analogies with quantum mechanics and {SIMD}-parallelism.},
	pages = {49--80},
	number = {1},
	journaltitle = {Information and Computation},
	shortjournal = {Information and Computation},
	author = {Asperti, Andrea and Mairson, Harry G.},
	date = {2001},
	file = {ScienceDirect Full Text PDF:/home/gabriele/.zotero/zotero/fuzoq98e.default/zotero/storage/G9TCZA8W/Asperti e Mairson - 2001 - Parallel Beta Reduction Is Not Elementary Recursiv.pdf:application/pdf;ScienceDirect Snapshot:/home/gabriele/.zotero/zotero/fuzoq98e.default/zotero/storage/ADXN3YAE/S089054010192869X.html:text/html}
}

@thesis{kathail_optimal_1990,
	title = {Optimal Interpreters for lambda-calculus based functional languages},
	institution = {Massachusetts Institute of Technology},
	type = {phdthesis},
	author = {Kathail, Vinod},
	date = {1990}
}

@article{asperti_bologna_1996,
	title = {The Bologna Optimal Higher-Order Machine},
	volume = {6},
	abstract = {The Bologna Optimal Higher-order Machine ({BOHM}) is a prototype implementation of the core of a functional language based on (a variant of) Lamping's optimal graph reduction technique (Lamping, 1990; Gonthier et al., 1992a; Asperti, 1994). The source language is a sugared λ-calculus enriched with booleans, integers, lists and basic operations on these data types (following the guidelines of Interaction Systems – Asperti and Laneve (1993b, 1994), Laneve (1993)). In this paper, we shall describe {BOHM}'s general architecture (comprising the garbage collector), and give a large set of benchmarks and experimental results.},
	pages = {763--810},
	number = {6},
	journaltitle = {Journal of Functional Programming},
	author = {Asperti, Andrea and Giovannetti, Cecilia and Naletto, Andrea},
	date = {1996},
	langid = {english},
	file = {Snapshot:/home/gabriele/.zotero/zotero/fuzoq98e.default/zotero/storage/9HM9ZA5Q/1F2763B0F931680F9B15BDC750BEB343.html:text/html}
}

@book{asperti_optimal_1998,
	title = {The Optimal Implementation of Functional Programming Languages},
	publisher = {Cambridge University Press},
	author = {Asperti, Andrea and Guerrini, Stefano},
	date = {1998}
}

@article{asperti_linear_1995,
	title = {Linear Logic, Comonads And Optimal Reductions},
	volume = {22},
	abstract = {The paper discusses, in a categorical perspective, some recent works on optimal graph reduction techniques for the λ-calculus. In particular, we relate the two “brackets” in [{GAL}92a] to the two operations associated with the comonad “!” of Linear Logic. The rewriting rules can be then understood as a “local implementation” of naturality laws, that is as the broadcasting of some information from the output to the inputs of a term, following its connected structure.},
	pages = {3--22},
	number = {1},
	journaltitle = {Fundam. Inf.},
	author = {Asperti, Andrea},
	date = {1995}
}

@inproceedings{gonthier_geometry_1992,
	title = {The Geometry of Optimal Lambda Reduction},
	abstract = {Lamping discovered an optimal graph-reduction implementation of the \&lgr;-calculus. Simultaneously, Girard invented the geometry of interaction, a mathematical foundation for operational semantics. In this paper, we connect and explain the geometry of interaction and Lamping's graphs. The geometry of interaction provides a suitable semantic basis for explaining and improving Lamping's system. On the other hand, graphs similar to Lamping's provide a concrete representation of the geometry of interaction. Together, they offer a new understanding of computation, as well as ideas for efficient and correct implementations.},
	pages = {15--26},
	booktitle = {Proceedings of the 19th {POPL}},
	publisher = {{ACM}},
	author = {Gonthier, Georges and Abadi, Martín and Lévy, Jean-Jacques},
	date = {1992}
}

@article{girard_linear_1987,
	title = {Linear logic},
	volume = {50},
	abstract = {The familiar connective of negation is broken into two operations: linear negation which is the purely negative part of negation and the modality “of course” which has the meaning of a reaffirmation. Following this basic discovery, a completely new approach to the whole area between constructive logics and programmation is initiated.},
	pages = {1--101},
	number = {1},
	journaltitle = {Theoretical Computer Science},
	shortjournal = {Theoretical Computer Science},
	author = {Girard, Jean-Yves},
	date = {1987},
	file = {ScienceDirect Full Text PDF:/home/gabriele/.zotero/zotero/fuzoq98e.default/zotero/storage/UBUVI3EX/Girard - 1987 - Linear logic.pdf:application/pdf;ScienceDirect Snapshot:/home/gabriele/.zotero/zotero/fuzoq98e.default/zotero/storage/3FVDMBUN/0304397587900454.html:text/html}
}

@incollection{girard_geometry_1989,
	title = {Geometry of Interaction 1: Interpretation of System F},
	volume = {127},
	shorttitle = {Geometry of Interaction 1},
	abstract = {This chapter describes the development of a semantics of computation free from the twin drawbacks of reductionism (that leads to static modification) and subjectivism (that leads to syntactical abuses, in other terms, bureaucracy). The new approach initiated in this chapter rests on the use of a specific C*-algebra Λ* that has the distinguished property of bearing a (non associative) inner tensor product. The chapter describes that a representative class of algorithms can be modelized by means of standard mathematics.},
	pages = {221--260},
	booktitle = {Studies in Logic and the Foundations of Mathematics},
	publisher = {Elsevier},
	author = {Girard, Jean-Yves},
	editor = {Ferro, R. and Bonotto, C. and Valentini, S. and Zanardo, A.},
	date = {1989},
	file = {ScienceDirect Snapshot:/home/gabriele/.zotero/zotero/fuzoq98e.default/zotero/storage/IMIC2WS6/S0049237X08702714.html:text/html}
}

@article{accattoli_leftmost-outermost_2016,
	title = {(Leftmost-Outermost) Beta Reduction is Invariant, Indeed},
	volume = {Volume 12, Issue 1},
	abstract = {Slot and van Emde Boas' weak invariance thesis states that reasonable machines can simulate each other within a polynomially overhead in time. Is lambda-calculus a reasonable machine? Is there a way to measure the computational complexity of a lambda-term? This paper presents the first complete positive answer to this long-standing problem. Moreover, our answer is completely machine-independent and based over a standard notion in the theory of lambda-calculus: the length of a leftmost-outermost derivation to normal form is an invariant cost model. Such a theorem cannot be proved by directly relating lambda-calculus with Turing machines or random access machines, because of the size explosion problem: there are terms that in a linear number of steps produce an exponentially long output. The first step towards the solution is to shift to a notion of evaluation for which the length and the size of the output are linearly related. This is done by adopting the linear substitution calculus ({LSC}), a calculus of explicit substitutions modeled after linear logic proof nets and admitting a decomposition of leftmost-outermost derivations with the desired property. Thus, the {LSC} is invariant with respect to, say, random access machines. The second step is to show that {LSC} is invariant with respect to the lambda-calculus. The size explosion problem seems to imply that this is not possible: having the same notions of normal form, evaluation in the {LSC} is exponentially longer than in the lambda-calculus. We solve such an impasse by introducing a new form of shared normal form and shared reduction, deemed useful. Useful evaluation avoids those steps that only unshare the output without contributing to beta-redexes, i.e. the steps that cause the blow-up in size. The main technical contribution of the paper is indeed the definition of useful reductions and the thorough analysis of their properties.},
	journaltitle = {Logical Methods in Computer Science},
	author = {Accattoli, Beniamino and Dal Lago, Ugo},
	date = {2016},
	langid = {english},
	file = {Full Text PDF:/home/gabriele/.zotero/zotero/fuzoq98e.default/zotero/storage/HJEMLXUF/Lago e Accattoli - 2016 - (Leftmost-Outermost) Beta Reduction is Invariant, .pdf:application/pdf;Snapshot:/home/gabriele/.zotero/zotero/fuzoq98e.default/zotero/storage/RHNM64UR/1627.html:text/html}
}

@article{asperti_about_2017,
	title = {About the efficient reduction of lambda terms},
	abstract = {There is still a lot of confusion about "optimal" sharing in the lambda calculus, and its actual efficiency. In this article, we shall try to clarify some of these issues.},
	journaltitle = {{arXiv}:1701.04240 [cs]},
	author = {Asperti, Andrea},
	date = {2017},
	keywords = {Computer Science - Logic in Computer Science},
	file = {arXiv\:1701.04240 PDF:/home/gabriele/.zotero/zotero/fuzoq98e.default/zotero/storage/AQWRZCVF/Asperti - 2017 - About the efficient reduction of lambda terms.pdf:application/pdf;arXiv.org Snapshot:/home/gabriele/.zotero/zotero/fuzoq98e.default/zotero/storage/8YUS5LDF/1701.html:text/html}
}

@inproceedings{guerrini_is_2017,
	title = {Is the Optimal Implementation Inefficient? Elementarily Not},
	volume = {84},
	series = {{LIPIcs}},
	pages = {17:1--17:16},
	booktitle = {2nd {FSCD}},
	author = {Guerrini, Stefano and Solieri, Marco},
	editor = {Miller, Dale},
	date = {2017},
	keywords = {complexity, lambda-calculus, linear logic, optimality, proof nets, sharing graphs},
	file = {Full Text PDF:/home/gabriele/.zotero/zotero/fuzoq98e.default/zotero/storage/64CCQQMV/Guerrini e Solieri - 2017 - Is the Optimal Implementation Inefficientl Element.pdf:application/pdf;Snapshot:/home/gabriele/.zotero/zotero/fuzoq98e.default/zotero/storage/CB3Q2MXE/7733.html:text/html}
}

@article{baillot_light_2011,
	title = {Light logics and optimal reduction: Completeness and complexity},
	volume = {209},
	shorttitle = {Light logics and optimal reduction},
	abstract = {Typing of lambda-terms in elementary and light affine logic ({EAL} and {LAL}, respectively) has been studied for two different reasons: on the one hand the evaluation of typed terms using {LAL} ({EAL}, respectively) proof-nets admits a guaranteed polynomial (elementary, respectively) bound; on the other hand these terms can also be evaluated by optimal reduction using the abstract version of Lamping’s algorithm. The first reduction is global while the second one is local and asynchronous. We prove that for {LAL} ({EAL}, respectively) typed terms, Lamping’s abstract algorithm also admits a polynomial (elementary, respectively) bound. We also give a proof of its soundness and completeness (for {EAL} and {LAL} with type fixpoints), by using a simple geometry of interaction model (context semantics).},
	pages = {118--142},
	number = {2},
	journaltitle = {Information and Computation},
	shortjournal = {Information and Computation},
	author = {Baillot, Patrick and Coppola, Paolo and Dal Lago, Ugo},
	date = {2011},
	keywords = {Implicit computational complexity, Lambda calculus, Light linear logic, Linear logic, Optimal reduction, Proof-nets},
	file = {ScienceDirect Full Text PDF:/home/gabriele/.zotero/zotero/fuzoq98e.default/zotero/storage/JYD3FN9Q/Baillot et al. - 2011 - Light logics and optimal reduction Completeness a.pdf:application/pdf;ScienceDirect Snapshot:/home/gabriele/.zotero/zotero/fuzoq98e.default/zotero/storage/EZSWHN68/S0890540110001604.html:text/html}
}

@article{hartmanis_computational_1965,
	title = {On the Computational Complexity of Algorithms},
	volume = {117},
	pages = {285--306},
	journaltitle = {Transactions of the American Mathematical Society},
	author = {Hartmanis, J. and Stearns, R. E.},
	date = {1965}
}

@article{backus_can_1978,
	title = {Can Programming Be Liberated from the Von Neumann Style? A Functional Style and Its Algebra of Programs},
	volume = {21},
	shorttitle = {Can Programming Be Liberated from the Von Neumann Style?},
	abstract = {Conventional programming languages are growing ever more enormous, but not stronger. Inherent defects at the most basic level cause them to be both fat and weak: their primitive word-at-a-time style of programming inherited from their common ancestor—the von Neumann computer, their close coupling of semantics to state transitions, their division of programming into a world of expressions and a world of statements, their inability to effectively use powerful combining forms for building new programs from existing ones, and their lack of useful mathematical properties for reasoning about programs.
An alternative functional style of programming is founded on the use of combining forms for creating programs. Functional programs deal with structured data, are often nonrepetitive and nonrecursive, are hierarchically constructed, do not name their arguments, and do not require the complex machinery of procedure declarations to become generally applicable. Combining forms can use high level programs to build still higher level ones in a style not possible in conventional languages.
Associated with the functional style of programming is an algebra of programs whose variables range over programs and whose operations are combining forms. This algebra can be used to transform programs and to solve equations whose “unknowns” are programs in much the same way one transforms equations in high school algebra. These transformations are given by algebraic laws and are carried out in the same language in which programs are written. Combining forms are chosen not only for their programming power but also for the power of their associated algebraic laws. General theorems of the algebra give the detailed behavior and termination conditions for large classes of programs.
A new class of computing systems uses the functional programming style both in its programming language and in its state transition rules. Unlike von Neumann languages, these systems have semantics loosely coupled to states—only one state transition occurs per major computation.},
	pages = {613--641},
	number = {8},
	journaltitle = {Commun. {ACM}},
	author = {Backus, John},
	date = {1978},
	keywords = {algebra of programs, applicative computing systems, applicative state transition systems, combining forms, functional forms, functional programming, metacomposition, models of computing systems, program correctness, program termination, program transformation, programming languages, von Neumann computers, von Neumann languages},
	file = {ACM Full Text PDF:/home/gabriele/.zotero/zotero/fuzoq98e.default/zotero/storage/55WVYGJR/Backus - 1978 - Can Programming Be Liberated from the Von Neumann .pdf:application/pdf}
}

@article{landin_correspondence_1965,
	title = {A Correspondence Between {ALGOL} 60 and Church's Lambda-notations: Part {II}},
	volume = {8},
	shorttitle = {A Correspondence Between {ALGOL} 60 and Church's Lambda-notations},
	pages = {158--167},
	number = {3},
	journaltitle = {Commun. {ACM}},
	author = {Landin, P. J.},
	date = {1965}
}

@article{landin_correspondence_1965-1,
	title = {A Correspondence Between {ALGOL} 60 and Church's Lambda-notation: Part I},
	volume = {8},
	shorttitle = {Correspondence Between {ALGOL} 60 and Church's Lambda-notation},
	pages = {89--101},
	number = {2},
	journaltitle = {Commun. {ACM}},
	author = {Landin, P. J.},
	date = {1965}
}

@article{xi_upper_1999,
	title = {Upper bounds for standardizations and an application},
	volume = {64},
	abstract = {{AbstractWe} present a new proof for the standardization theorem in λ-calculus, which is largely built upon a structural induction on λ-terms. We then extract some bounds for the number of β-reduction steps in the standard β-reduction sequence obtained from transforming a given β-reduction sequence, sharpening the standardization theorem. As an application, we establish a super exponential bound for the lengths of β-reduction sequences from any given simply typed λ-terms.},
	pages = {291--303},
	number = {1},
	journaltitle = {The Journal of Symbolic Logic},
	author = {Xi, Hongwei},
	date = {1999},
	langid = {english}
}

@article{backus_syntax_1959,
	title = {The syntax and semantics of the proposed international algebraic language of the Zurich {ACM}-{GAMM} Conference},
	abstract = {This paper gives a tutorial summary of the syntax and interpretation rules of the proposed international algebraic language put forward by the Zurich {ACM}-{GAMM} Conference, followed by a formal, complete presentation of the same information. Notations are presented for numbers, numerical variables, Boolean variables, relations, n-dimensional arrays, functi ons, operator s and algebraic expre s sions. Means are provided in the language for specifying assignment of values to. variables, conditional execution of statements, iterative proce\&lt;i;ures, formation of compound statements from sequences of statements, definition of new statements for arbitrary procedures, reuse and alteration of program segments. The proposed language is intended to provide convenient and concise means for expressing virtually all procedures of {numericaL} computation while employing relatively few syntactical rules and statement types.},
	pages = {125--132},
	journaltitle = {Proc. Int. Conf. on  Information Processing},
	author = {Backus, John},
	date = {1959},
	file = {Full Text PDF:/home/gabriele/.zotero/zotero/fuzoq98e.default/zotero/storage/FVE99LSV/Backus - 1959 - The syntax and semantics of the proposed internati.pdf:application/pdf;Snapshot:/home/gabriele/.zotero/zotero/fuzoq98e.default/zotero/storage/Q9QNGTDM/790ea9d7f910ef2def2418b44a69b111d9f8ceec.html:text/html}
}

@article{mitschke_standardization_1979,
	title = {The Standardization Theorem for {$\lambda$}-Calculus},
	volume = {25},
	pages = {29--31},
	number = {1},
	journaltitle = {Zeitschrift für Mathematische Logik und Grundlagen der Mathematik},
	author = {Mitschke, Gerd},
	date = {1979},
	langid = {german}
}

@article{sinot_sub-lambda-calculi_2008,
	title = {Sub-{$\lambda$}-calculi, Classified},
	volume = {203},
	series = {Proceedings of 4th {TERMGRAPH}},
	abstract = {When sharing is studied in the λ-calculus, some sub-calculi often pop up, for instance λI or the linear λ-calculus. In this paper, we generalise these to a large class of sub-calculi, parametrised by an arbitrary predicate on the number of occurrences of bound variables. Such a definition only makes sense when the sub-calculi are stable by β-reduction. Surprisingly, we are able to give a complete description and classification of such stable sub-calculi, in a rather algebraic way; and surprisingly again, we discover some unexpected such subcalculi. This could lead to a better understanding of the structure of the λ-calculus.},
	pages = {123--133},
	number = {1},
	journaltitle = {Electronic Notes in Theoretical Computer Science},
	author = {Sinot, François-Régis},
	date = {2008},
	keywords = {-calculus, linearity, sharing}
}

@book{barendregt_lambda_1984,
	title = {The lambda calculus: its syntax and semantics},
	shorttitle = {The lambda calculus},
	abstract = {The revised edition contains a new chapter which provides an elegant description of the semantics. The various classes of lambda calculus models are described in a uniform manner. Some didactical improvements have been made to this edition. An example of a simple model is given and then the general theory (of categorical models) is developed. Indications are given of those parts of the book which can be used to form a coherent course.},
	publisher = {North-Holland},
	author = {Barendregt, Hendrik Pieter},
	date = {1984},
	langid = {english},
	keywords = {Mathematics / Logic, Lambda calculus, Mathematics / Calculus, Mathematics / Reference}
}

@book{terese_term_2003,
	title = {Term Rewriting Systems},
	abstract = {Term rewriting systems, which developed out of mathematical logic, consist of sequences of discrete steps where one term is replaced with another. Their many applications range from automatic theorem proving systems to computer algebra. This book begins with several examples, followed by a chapter on basic notions that provides a foundation for the rest of the work. First-order and higher-order theories are presented, with much of the latter material appearing for the first time in book form. Subjects treated include orthogonality, termination, lambda calculus and term graph rewriting. There is also a chapter detailing the required mathematical background.},
	publisher = {Cambridge University Press},
	author = {{Terese}},
	date = {2003}
}

@book{baader_term_1999,
	title = {Term Rewriting and All That},
	abstract = {This textbook offers a unified, self-contained introduction to the field of term rewriting. Baader and Nipkow cover all the basic material–abstract reduction systems, termination, confluence, completion, and combination problems–but also some important and closely connected subjects: universal algebra, unification theory, Gröbner bases, and Buchberger's algorithm. They present the main algorithms both informally and as programs in the functional language Standard {ML} (An appendix contains a quick and easy introduction to {ML}). Key chapters cover crucial algorithms such as unification and congruence closure in more depth and develop efficient Pascal programs. The book contains many examples and over 170 exercises. This is also an ideal reference book for professional researchers: results spread over many conference and journal articles are collected here in a unified notation, detailed proofs of almost all theorems are provided, and each chapter closes with a guide to the literature.},
	publisher = {Cambridge University Press},
	author = {Baader, Franz and Nipkow, Tobias},
	date = {1999}
}

@book{ash_probability_1999,
	location = {San Diego},
	edition = {2nd edition},
	title = {Probability and Measure Theory},
	abstract = {Probability and Measure Theory, Second Edition, is a text for a graduate-level course in probability that includes essential background topics in analysis. It provides extensive coverage of conditional probability and expectation, strong laws of large numbers, martingale theory, the central limit theorem, ergodic theory, and Brownian motion.Clear, readable {styleSolutions} to many problems presented in {textSolutions} manual for {instructorsMaterial} new to the second edition on ergodic theory, Brownian motion, and convergence theorems used in {statisticsNo} knowledge of general topology required, just basic analysis and metric {spacesEfficient} organization},
	publisher = {Academic Press},
	author = {Ash, Robert B. and Doléans-Dade, Catherine A.},
	date = {1999}
}

@book{motwani_randomized_1995,
	title = {Randomized Algorithms},
	abstract = {This text by two well-known experts in the field presents the basic concepts in the design and analysis of randomized algorithms at a level accessible to beginning graduate students, professionals and researchers.},
	publisher = {Cambridge University Press},
	author = {Motwani, Rajeev and Raghavan, Prabhakar},
	date = {1995}
}

@book{bremaud_markov_1999,
	title = {Markov Chains: Gibbs Fields, Monte Carlo Simulation, and Queues},
	shorttitle = {Markov Chains},
	abstract = {In this book, the author begins with the elementary theory of Markov chains and very progressively brings the reader to the more advanced topics. He gives a useful review of probability that makes the book self-contained, and provides an appendix with detailed proofs of all the prerequisites from calculus, algebra, and number theory. A number of carefully chosen problems of varying difficulty are proposed at the close of each chapter, and the mathematics are slowly and carefully developed, in order to make self-study easier. The author treats the classic topics of Markov chain theory, both in discrete time and continuous time, as well as the connected topics such as finite Gibbs fields, nonhomogeneous Markov chains, discrete- time regenerative processes, Monte Carlo simulation, simulated annealing, and queuing theory. The result is an up-to-date textbook on stochastic processes. Students and researchers in operations research and electrical engineering, as well as in physics and biology, will find it very accessible and relevant.},
	publisher = {Springer-Verlag},
	author = {Bremaud, Pierre},
	date = {1999},
	langid = {english}
}

@book{norris_markov_1998,
	title = {Markov Chains},
	abstract = {Markov chains are central to the understanding of random processes. This is not only because they pervade the applications of random processes, but also because one can calculate explicitly many quantities of interest. This textbook, aimed at advanced undergraduate or {MSc} students with some background in basic probability theory, focuses on Markov chains and quickly develops a coherent and rigorous theory whilst showing also how actually to apply it. Both discrete-time and continuous-time chains are studied. A distinguishing feature is an introduction to more advanced topics such as martingales and potentials in the established context of Markov chains. There are applications to simulation, economics, optimal control, genetics, queues and many other topics, and exercises and examples drawn both from theory and practice. It will therefore be an ideal text either for elementary courses on random processes or those that are more oriented towards applications.},
	publisher = {Cambridge University Press},
	author = {Norris, J. R.},
	date = {1998},
	langid = {english},
	keywords = {Mathematics / Probability \& Statistics / General}
}

@article{rabin_probabilistic_1980,
	title = {Probabilistic algorithm for testing primality},
	volume = {12},
	abstract = {We present a practical probabilistic algorithm for testing large numbers of arbitrary form for primality. The algorithm has the feature that when it determines a number composite then the result is always true, but when it asserts that a number is prime there is a provably small probability of error. The algorithm was used to generate large numbers asserted to be primes of arbitrary and special forms, including very large numbers asserted to be twin primes. Theoretical foundations as well as details of implementation and experimental results are given.},
	pages = {128--138},
	number = {1},
	journaltitle = {Journal of Number Theory},
	shortjournal = {Journal of Number Theory},
	author = {Rabin, Michael O.},
	date = {1980},
	file = {ScienceDirect Full Text PDF:/home/gabriele/.zotero/zotero/fuzoq98e.default/zotero/storage/GTWH6N77/Rabin - 1980 - Probabilistic algorithm for testing primality.pdf:application/pdf;ScienceDirect Snapshot:/home/gabriele/.zotero/zotero/fuzoq98e.default/zotero/storage/EBCHYXPR/0022314X80900840.html:text/html}
}

@article{de_bruijn_lambda_1972,
	title = {Lambda calculus notation with nameless dummies, a tool for automatic formula manipulation, with application to the Church-Rosser theorem},
	volume = {75},
	abstract = {In ordinary lambda calculus the occurrences of a bound variable are made recognizable by the use of one and the same (otherwise irrelevant) name at all occurrences. This convention is known to cause considerable trouble in cases of substitution. In the present paper a different notational system is developed, where occurrences of variables are indicated by integers giving the “distance” to the binding λ instead of a name attached to that λ. The system is claimed to be efficient for automatic formula manipulation as well as for metalingual discussion. As an example the most essential part of a proof of the Church-Rosser theorem is presented in this namefree calculus.},
	pages = {381--392},
	number = {5},
	journaltitle = {Indagationes Mathematicae (Proceedings)},
	shortjournal = {Indagationes Mathematicae (Proceedings)},
	author = {de Bruijn, N. G},
	date = {1972},
	file = {ScienceDirect Full Text PDF:/home/gabriele/.zotero/zotero/fuzoq98e.default/zotero/storage/LNMXKRMM/de Bruijn - 1972 - Lambda calculus notation with nameless dummies, a .pdf:application/pdf;ScienceDirect Snapshot:/home/gabriele/.zotero/zotero/fuzoq98e.default/zotero/storage/5CLST2AW/1385725872900340.html:text/html}
}

@inproceedings{goldreich_how_1984,
	title = {How To Construct Random Functions},
	abstract = {This paper develops a constructive theory of randomness for functions based on computational complexity. We present a deterministic polynomial-time algorithm that transforms pairs (g,r), where g is any one-way (in a very weak sense) function and r is a random k-bit string, to polynomial-time computable functions f/sub r/:1,..., 2/sup k /spl I.oarr/ 1, ..., 2/sup k/. These f/sub r/'s cannot be distinguished from random functions by any probabilistic polynomial time algorithm that asks and receives the value of a function at arguments of its choice. The result has applications in cryptography, random constructions and complexity theory.},
	pages = {464--479},
	booktitle = {Proceedings of 25th {FOCS}.},
	author = {Goldreich, O. and Goldwasser, S. and Micali, S.},
	date = {1984},
	keywords = {Polynomials, Computational complexity, Cryptography, Indexing, Reflection},
	file = {IEEE Xplore Abstract Record:/home/gabriele/.zotero/zotero/fuzoq98e.default/zotero/storage/EFGDMDNT/715949.html:text/html}
}

@inproceedings{goodman_church:_2008,
	title = {Church: A Language for Generative Models},
	shorttitle = {Church},
	abstract = {Formal languages for probabilistic modeling enable re-use, modularity, and descriptive clarity, and can foster generic inference techniques. We introduce Church, a universal language for describing stochastic generative processes. Church is based on the Lisp model of lambda calculus, containing a pure Lisp as its deterministic subset. The semantics of Church is defined in terms of evaluation histories and conditional distributions on such histories. Church also includes a novel language construct, the stochastic memoizer, which enables simple description of many complex non-parametric models. We illustrate language features through several examples, including: a generalized Bayes net in which parameters cluster over trials, infinite {PCFGs}, planning by inference, and various non-parametric clustering models. Finally, we show how to implement query on any Church program, exactly and approximately, using Monte Carlo techniques.},
	pages = {220--229},
	booktitle = {Proceedings of the 24th Conference on Uncertainty in Artificial Intelligence},
	publisher = {{AUAI} Press},
	author = {Goodman, Noah D. and Mansinghka, Vikash K. and Roy, Daniel and Bonawitz, Keith and Tenenbaum, Joshua B.},
	date = {2008}
}

@article{church_unsolvable_1936,
	title = {An Unsolvable Problem of Elementary Number Theory},
	volume = {58},
	pages = {345--363},
	number = {2},
	journaltitle = {American Journal of Mathematics},
	author = {Church, Alonzo},
	date = {1936}
}

@article{dal_lago_invariant_2005,
	title = {An Invariant Cost Model for the Lambda Calculus},
	abstract = {We define a new cost model for the call-by-value lambda-calculus satisfying the invariance thesis. That is, under the proposed cost model, Turing machines and the call-by-value lambda-calculus can simulate each other within a polynomial time overhead. The model only relies on combinatorial properties of usual beta-reduction, without any reference to a specific machine or evaluator. In particular, the cost of a single beta reduction is proportional to the difference between the size of the redex and the size of the reduct. In this way, the total cost of normalizing a lambda term will take into account the size of all intermediate results (as well as the number of steps to normal form).},
	journaltitle = {{arXiv}:cs/0511045},
	author = {Dal Lago, Ugo and Martini, Simone},
	date = {2005},
	keywords = {Computer Science - Computational Complexity, Computer Science - Logic in Computer Science, F.4.1},
	file = {arXiv\:cs/0511045 PDF:/home/gabriele/.zotero/zotero/fuzoq98e.default/zotero/storage/L5BUHRGJ/Lago e Martini - 2005 - An Invariant Cost Model for the Lambda Calculus.pdf:application/pdf;arXiv.org Snapshot:/home/gabriele/.zotero/zotero/fuzoq98e.default/zotero/storage/TQF38K77/0511045.html:text/html}
}

@inproceedings{wood_new_2014,
	title = {A New Approach to Probabilistic Programming Inference},
	abstract = {We introduce and demonstrate a new approach to inference in expressive probabilistic programming languages based on particle Markov chain Monte Carlo. Our approach is easy to implement and to paral...},
	pages = {1024--1032},
	booktitle = {Artificial Intelligence and Statistics},
	author = {Wood, Frank and Meent, Jan Willem and Mansinghka, Vikash},
	date = {2014},
	langid = {english},
	file = {Full Text PDF:/home/gabriele/.zotero/zotero/fuzoq98e.default/zotero/storage/S4RUK3DJ/Wood et al. - 2014 - A New Approach to Probabilistic Programming Infere.pdf:application/pdf;Snapshot:/home/gabriele/.zotero/zotero/fuzoq98e.default/zotero/storage/QG47TELQ/wood14.html:text/html}
}

@inproceedings{borgstrom_lambda-calculus_2016,
	title = {A Lambda-calculus Foundation for Universal Probabilistic Programming},
	abstract = {We develop the operational semantics of an untyped probabilistic λ-calculus with continuous distributions, and both hard and soft constraints,as a foundation for universal probabilistic programming languages such as Church, Anglican, and Venture. Our first contribution is to adapt the classic operational semantics of λ-calculus to a continuous setting via creating a measure space on terms and defining step-indexed approximations. We prove equivalence of big-step and small-step formulations of this distribution-based semantics. To move closer to inference techniques, we also define the sampling-based semantics of a term as a function from a trace of random samples to a value. We show that the distribution induced by integration over the space of traces equals the distribution-based semantics. Our second contribution is to formalize the implementation technique of trace Markov chain Monte Carlo ({MCMC}) for our calculus and to show its correctness. A key step is defining sufficient conditions for the distribution induced by trace {MCMC} to converge to the distribution-based semantics. To the best of our knowledge, this is the first rigorous correctness proof for trace {MCMC} for a higher-order functional language, or for a language with soft constraints.},
	pages = {33--46},
	booktitle = {Proc. of the 21st {ICFP}},
	author = {Borgström, Johannes and Dal Lago, Ugo and Gordon, Andrew D. and Szymczak, Marcin},
	date = {2016},
	keywords = {Lambda-calculus, Machine Learning, {MCMC}, Operational Semantics, Probabilistic Programming}
}

@inproceedings{staton_semantics_2016,
	title = {Semantics for Probabilistic Programming: Higher-order Functions, Continuous Distributions, and Soft Constraints},
	shorttitle = {Semantics for Probabilistic Programming},
	abstract = {We study the semantic foundation of expressive probabilistic programming languages, that support higher-order functions, continuous distributions, and soft constraints (such as Anglican, Church, and Venture). We define a metalanguage (an idealised version of Anglican) for probabilistic computation with the above features, develop both operational and denotational semantics, and prove soundness, adequacy, and termination. This involves measure theory, stochastic labelled transition systems, and functor categories, but admits intuitive computational readings, one of which views sampled random variables as dynamically allocated read-only variables. We apply our semantics to validate nontrivial equations underlying the correctness of certain compiler optimisations and inference algorithms such as sequential Monte Carlo simulation. The language enables defining probability distributions on higher-order functions, and we study their properties.},
	pages = {525--534},
	booktitle = {Proceedings of the 31st {LICS}},
	publisher = {{ACM}},
	author = {Staton, Sam and Yang, Hongseok and Wood, Frank and Heunen, Chris and Kammar, Ohad},
	date = {2016},
	file = {ACM Full Text PDF:/home/gabriele/.zotero/zotero/fuzoq98e.default/zotero/storage/VQJJ5FCL/Staton et al. - 2016 - Semantics for Probabilistic Programming Higher-or.pdf:application/pdf}
}

@online{noauthor_typed_nodate,
	title = {Typed Lambda Calculi and Applications},
	url = {https://www.mimuw.edu.pl/tlca/tlca.html},
	urldate = {2018-08-17},
	file = {:/home/gabriele/.zotero/zotero/fuzoq98e.default/zotero/storage/A328MQM2/tlca.html:text/html}
}

@online{noauthor_rewriting_nodate,
	title = {Rewriting Techniques and Applications},
	url = {http://rewriting.loria.fr/rta/},
	urldate = {2018-08-17},
	file = {RTA:/home/gabriele/.zotero/zotero/fuzoq98e.default/zotero/storage/DJHBZV9Q/rta.html:text/html}
}

@article{dal_lago_randomised_2018,
	title = {On Randomised Strategies in the {$\lambda$}-Calculus (Long Version)},
	url = {Available at: http://arxiv.org/abs/1805.03934},
	abstract = {In this work we introduce randomised reduction strategies, a notion already studied in the context of abstract reduction systems, for the \${\textbackslash}textbackslashlambda\$-calculus. We develop a simple framework that allows us to prove if a probabilistic strategy is positive almost-surely normalising. Then we propose a simple example of probabilistic strategy for the \${\textbackslash}textbackslashlambda\$-calculus that has such a property and we show why it is non-trivial with respect to classical deterministic strategies such as leftmost-outermost or rightmost-innermost. We conclude studying this strategy for two classical sub-\${\textbackslash}textbackslashlambda\$-calculi, namely those in which duplication and erasure are syntactically forbidden.},
	author = {Dal Lago, Ugo and Vanoni, Gabriele},
	urldate = {2018-05-28},
	date = {2018},
	keywords = {Computer Science - Logic in Computer Science}
}

@book{klop_combinatory_1980,
	title = {Combinatory Reduction Systems},
	publisher = {Mathematisch Centrum},
	author = {Klop, J. W.},
	date = {1980}
}

@inproceedings{bournez_probabilistic_2002,
	title = {Probabilistic Rewrite Strategies. Applications to {ELAN}},
	volume = {2378},
	series = {{LNCS}},
	abstract = {Recently rule based languages focussed on the use of rewriting as a modeling tool which results in making specifications executable. To extend the modeling capabilities of rule based languages, we explore the possibility of making the rule applications subject to probabilistic choices.},
	pages = {252--266},
	booktitle = {Proceedings of 13th {RTA}},
	publisher = {Springer},
	author = {Bournez, Olivier and Kirchner, Claude},
	date = {2002}
}

@book{curry_combinatory_1958,
	title = {Combinatory Logic},
	publisher = {North-Holland},
	author = {Curry, Haskell Brooks and Feys, Robert},
	date = {1958},
	langid = {english}
}

@inproceedings{avanzini_probabilistic_2018,
	title = {On Probabilistic Term Rewriting},
	volume = {10818},
	series = {{LNCS}},
	abstract = {We study the termination problem for probabilistic term rewrite systems. We prove that the interpretation method is sound and complete for a strengthening of positive almost sure termination, when abstract reduction systems and term rewrite systems are considered. Two instances of the interpretation method—polynomial and matrix interpretations—are analyzed and shown to capture interesting and nontrivial examples when automated. We capture probabilistic computation in a novel way by means of multidistribution reduction sequences, thus accounting for both the nondeterminism in the choice of the redex and the probabilism intrinsic in firing each rule.},
	pages = {132--148},
	booktitle = {Proceedings of 14th {FLOPS}},
	publisher = {Springer},
	author = {Avanzini, Martin and Dal Lago, Ugo and Yamada, Akihisa},
	date = {2018},
	langid = {english}
}

@article{zantema_strategy_2012,
	title = {Strategy Independent Reduction Lengths in Rewriting and Binary Arithmetic},
	volume = {82},
	abstract = {In this paper we give a criterion by which one can conclude that every reduction of a basic term to normal form has the same length. As a consequence, the number of steps to reach the normal form is independent of the chosen strategy. In particular this holds for {TRSs} computing addition and multiplication of natural numbers, both in unary and binary notation.},
	pages = {69--76},
	journaltitle = {{EPTCS}},
	author = {Zantema, Hans},
	date = {2012},
	keywords = {Computer Science - Logic in Computer Science, Computer Science - Programming Languages}
}

@inproceedings{ferrer_fioriti_probabilistic_2015,
	title = {Probabilistic Termination: Soundness, Completeness, and Compositionality},
	shorttitle = {Probabilistic Termination},
	abstract = {We propose a framework to prove almost sure termination for probabilistic programs with real valued variables. It is based on ranking supermartingales, a notion analogous to ranking functions on non-probabilistic programs. The framework is proven sound and complete for a meaningful class of programs involving randomization and bounded nondeterminism. We complement this foundational insigh by a practical proof methodology, based on sound conditions that enable compositional reasoning and are amenable to a direct implementation using modern theorem provers. This is integrated in a small dependent type system, to overcome the problem that lexicographic ranking functions fail when combined with randomization. Among others, this compositional methodology enables the verification of probabilistic programs outside the complete class that admits ranking supermartingales.},
	pages = {489--501},
	booktitle = {Proceedings of 42nd {POPL}},
	publisher = {{ACM}},
	author = {Ferrer Fioriti, Luis María and Hermanns, Holger},
	date = {2015},
	keywords = {probabilistic programs, program verification, supermartingales, termination}
}

@inproceedings{bournez_proving_2005,
	title = {Proving Positive Almost-Sure Termination},
	volume = {3467},
	series = {{LNCS}},
	abstract = {In order to extend the modeling capabilities of rewriting systems, it is rather natural to consider that the firing of rules can be subject to some probabilistic laws. Considering rewrite rules subject to probabilities leads to numerous questions about the underlying notions and results.We focus here on the problem of termination of a set of probabilistic rewrite rules. A probabilistic rewrite system is said almost surely terminating if the probability that a derivation leads to a normal form is one. Such a system is said positively almost surely terminating if furthermore the mean length of a derivation is finite. We provide several results and techniques in order to prove positive almost sure termination of a given set of probabilistic rewrite rules. All these techniques subsume classical ones for non-probabilistic systems.},
	pages = {323--337},
	booktitle = {Proceedings of 16th {RTA}},
	publisher = {Springer},
	author = {Bournez, Olivier and Garnier, Florent},
	date = {2005},
	langid = {english}
}

@article{rabin_probabilistic_1963,
	title = {Probabilistic automata},
	volume = {6},
	pages = {230--245},
	number = {3},
	journaltitle = {Information and Control},
	shortjournal = {Information and Control},
	author = {Rabin, Michael O.},
	date = {1963},
	file = {ScienceDirect Full Text PDF:/home/gabriele/.zotero/zotero/fuzoq98e.default/zotero/storage/VYAU49YY/Rabin - 1963 - Probabilistic automata.pdf:application/pdf;ScienceDirect Snapshot:/home/gabriele/.zotero/zotero/fuzoq98e.default/zotero/storage/JG2LYSIH/S0019995863902900.html:text/html}
}

@article{church_set_1932,
	title = {A Set of Postulates for the Foundation of Logic},
	volume = {33},
	pages = {346--366},
	number = {2},
	journaltitle = {Annals of Mathematics},
	author = {Church, Alonzo},
	date = {1932}
}

@article{kleene_inconsistency_1935,
	title = {The Inconsistency of Certain Formal Logics},
	volume = {36},
	pages = {630--636},
	number = {3},
	journaltitle = {Annals of Mathematics},
	author = {Kleene, S. C. and Rosser, J. B.},
	date = {1935}
}

@article{church_note_1936,
	title = {A Note on the Entscheidungsproblem},
	volume = {1},
	pages = {40--41},
	number = {1},
	journaltitle = {Journal of Symbolic Logic},
	author = {Church, Alonzo},
	date = {1936}
}

@article{turing_computability_1937,
	title = {Computability and {$\lambda$}\$-Definability},
	volume = {2},
	pages = {153--163},
	number = {4},
	journaltitle = {Journal of Symbolic Logic},
	shortjournal = {J. Symbolic Logic},
	author = {Turing, A. M.},
	date = {1937},
	file = {Snapshot:/home/gabriele/.zotero/zotero/fuzoq98e.default/zotero/storage/NRZS4HXV/1183383711.html:text/html}
}

@article{kleene_$lambda$-definability_1936,
	title = {{$\lambda$}-definability and recursiveness},
	volume = {2},
	pages = {340--353},
	number = {2},
	journaltitle = {Duke Mathematical Journal},
	shortjournal = {Duke Math. J.},
	author = {Kleene, S. C.},
	date = {1936},
	langid = {english},
	file = {Snapshot:/home/gabriele/.zotero/zotero/fuzoq98e.default/zotero/storage/36DVZ6LH/1077489488.html:text/html}
}

@book{frege_basic_1893,
	title = {The Basic Laws of Arithmetic: Exposition of the System},
	shorttitle = {The Basic Laws of Arithmetic},
	author = {Frege, Gottlob},
	date = {1893},
	langid = {english}
}

@article{schonfinkel_uber_1924,
	title = {Über die Bausteine der mathematischen Logik},
	volume = {92},
	pages = {305--316},
	journaltitle = {Mathematische Annalen},
	author = {Schönfinkel, Moses},
	date = {1924},
	langid = {german},
	file = {Snapshot:/home/gabriele/.zotero/zotero/fuzoq98e.default/zotero/storage/IS5MKWEW/img.html:text/html}
}

@article{landin_mechanical_1964,
	title = {The Mechanical Evaluation of Expressions},
	volume = {6},
	abstract = {Abstract.  This paper is a contribution to the “theory” of the activity of using computers. It shows how some forms of expression used in current programming la},
	pages = {308--320},
	number = {4},
	journaltitle = {The Computer Journal},
	shortjournal = {Comput J},
	author = {Landin, P. J.},
	date = {1964},
	langid = {english},
	file = {Full Text PDF:/home/gabriele/.zotero/zotero/fuzoq98e.default/zotero/storage/E7LKE7AH/Landin - 1964 - The Mechanical Evaluation of Expressions.pdf:application/pdf;Snapshot:/home/gabriele/.zotero/zotero/fuzoq98e.default/zotero/storage/7ZTLPG66/375725.html:text/html}
}

@article{plotkin_call-by-name_1975,
	title = {Call-by-name, call-by-value and the {$\lambda$}-calculus},
	volume = {1},
	abstract = {This paper examines the old question of the relationship between {ISWIM} and the λ-calculus, using the distinction between call-by-value and call-by-name. It is held that the relationship should be mediated by a standardisation theorem. Since this leads to difficulties, a new λ-calculus is introduced whose standardisation theorem gives a good correspondence with {ISWIM} as given by the {SECD} machine, but without the letrec feature. Next a call-by-name variant of {ISWIM} is introduced which is in an analogous correspondence withthe usual λ-calculus. The relation between call-by-value and call-by-name is then studied by giving simulations of each language by the other and interpretations of each calculus in the other. These are obtained as another application of the continuation technique. Some emphasis is placed throughout on the notion of operational equality (or contextual equality). If terms can be proved equal in a calculus they are operationally equal in the corresponding language. Unfortunately, operational equality is not preserved by either of the simulations.},
	pages = {125--159},
	number = {2},
	journaltitle = {Theoretical Computer Science},
	shortjournal = {Theoretical Computer Science},
	author = {Plotkin, Gordon D.},
	date = {1975},
	file = {ScienceDirect Full Text PDF:/home/gabriele/.zotero/zotero/fuzoq98e.default/zotero/storage/BLD6HJJ5/Plotkin - 1975 - Call-by-name, call-by-value and the λ-calculus.pdf:application/pdf;ScienceDirect Snapshot:/home/gabriele/.zotero/zotero/fuzoq98e.default/zotero/storage/8XMWUJFQ/0304397575900171.html:text/html}
}

@article{landin_next_1966,
	title = {The Next 700 Programming Languages},
	volume = {9},
	abstract = {A family of unimplemented computing languages is described that is intended to span differences of application area by a unified framework. This framework dictates the rules about the uses of user-coined names, and the conventions about characterizing functional relationships. Within this framework the design of a specific language splits into two independent parts. One is the choice of written appearances of programs (or more generally, their physical representation). The other is the choice of the abstract entities (such as numbers, character-strings, list of them, functional relations among them) that can be referred to in the language.
The system is biased towards “expressions” rather than “statements.” It includes a nonprocedural (purely functional) subsystem that aims to expand the class of users' needs that can be met by a single print-instruction, without sacrificing the important properties that make conventional right-hand-side expressions easy to construct and understand.},
	pages = {157--166},
	number = {3},
	journaltitle = {Commun. {ACM}},
	author = {Landin, P. J.},
	date = {1966}
}

@article{hindley_principal_1969,
	title = {The Principal Type-Scheme of an Object in Combinatory Logic},
	volume = {146},
	pages = {29--60},
	journaltitle = {Transactions of the American Mathematical Society},
	author = {Hindley, R.},
	date = {1969}
}

@article{milner_theory_1978,
	title = {A theory of type polymorphism in programming},
	volume = {17},
	abstract = {The aim of this work is largely a practical one. A widely employed style of programming, particularly in structure-processing languages which impose no discipline of types, entails defining procedures which work well on objects of a wide variety. We present a formal type discipline for such polymorphic procedures in the context of a simple pro-gramming language, and a compile time type-checking algorithm w which enforces the discipline. A Semantic Soundness Theorem (based on a formal semantics for the language) states that well-type programs cannot “go wrong ” and a Syntactic Soundness Theorem states that if fl accepts a program then it is well typed. We also discuss extending these results to richer languages; a type-checking algorithm based on w is in fact already implemented and working, for the metalanguage {ML} in the Edinburgh {LCF} system, 1.},
	pages = {348--375},
	journaltitle = {Journal of Computer and System Sciences},
	author = {Milner, Robin},
	date = {1978},
	file = {Citeseer - Full Text PDF:/home/gabriele/.zotero/zotero/fuzoq98e.default/zotero/storage/3JE8692L/Milner - 1978 - A theory of type polymorphism in programming.pdf:application/pdf;Citeseer - Snapshot:/home/gabriele/.zotero/zotero/fuzoq98e.default/zotero/storage/RSSEGEHF/summary.html:text/html}
}

@article{moggi_notions_1991,
	title = {Notions of computation and monads},
	volume = {93},
	abstract = {The λ-calculus is considered a useful mathematical tool in the study of programming languages, since programs can be identified with λ-terms. However, if one goes further and uses βη-conversion to prove equivalence of programs, then a gross simplification is introduced (programs are identified with total functions from values to values) that may jeopardise the applicability of theoretical results. In this paper we introduce calculi, based on a categorical semantics for computations, that provide a correct basis for proving equivalence of programs for a wide range of notions of computation.},
	pages = {55--92},
	number = {1},
	journaltitle = {Information and Computation},
	shortjournal = {Information and Computation},
	author = {Moggi, Eugenio},
	date = {1991},
	file = {ScienceDirect Full Text PDF:/home/gabriele/.zotero/zotero/fuzoq98e.default/zotero/storage/HH7PNLA3/Moggi - 1991 - Notions of computation and monads.pdf:application/pdf;ScienceDirect Snapshot:/home/gabriele/.zotero/zotero/fuzoq98e.default/zotero/storage/BKKVBY7Q/0890540191900524.html:text/html}
}

@incollection{dal_lago_short_2012,
	title = {A Short Introduction to Implicit Computational Complexity},
	volume = {7388},
	series = {{LNCS}},
	abstract = {These lecture notes are meant to serve as a short introduction to implicit computational complexity for those students who have little or no knowledge of recursion theory and proof theory. They have been obtained by enriching and polishing a set of notes the author wrote for a course (on the same subject) he gave at {ESSLLI} 2010. These notes are definitely not meant to be comprehensive nor exhaustive, but on the other hand much effort has been done to keep them self-contained.},
	pages = {89--109},
	booktitle = {Lectures on Logic and Computation: {ESSLLI} 2010},
	publisher = {Springer},
	author = {Dal Lago, Ugo},
	editor = {Bezhanishvili, Nick and Goranko, Valentin},
	date = {2012},
	langid = {english}
}

@inproceedings{baillot_soft_2004,
	title = {Soft lambda-Calculus: A Language for Polynomial Time Computation},
	volume = {2987},
	series = {{LNCS}},
	shorttitle = {Soft lambda-Calculus},
	abstract = {Soft linear logic ([Lafont02]) is a subsystem of linear logic characterizing the class {PTIME}. We introduce Soft lambda-calculus as a calculus typable in the intuitionistic and affine variant of this logic. We prove that the (untyped) terms of this calculus are reducible in polynomial time. We then extend the type system of Soft logic with recursive types. This allows us to consider non-standard types for representing lists. Using these datatypes we examine the concrete expressiveness of Soft lambda-calculus with the example of the insertion sort algorithm.},
	pages = {27--41},
	booktitle = {Proceedings of {FoSSaCS} 2004},
	publisher = {Springer},
	author = {Baillot, Patrick and Mogbil, Virgile},
	editor = {Walukiewicz, Igor},
	date = {2004},
	langid = {english},
	keywords = {Linear Logic, Polynomial Time, Reduction Rule, Reduction Sequence, Typing Rule},
	file = {Springer Full Text PDF:/home/gabriele/.zotero/zotero/fuzoq98e.default/zotero/storage/TJH9QMI6/Baillot e Mogbil - 2004 - Soft lambda-Calculus A Language for Polynomial Ti.pdf:application/pdf}
}

@article{terui_light_2007,
	title = {Light affine lambda calculus and polynomial time strong normalization},
	volume = {46},
	abstract = {Light Linear Logic ({LLL}) and Intuitionistic Light Affine Logic ({ILAL}) are logics that capture polynomial time computation. It is known that every polynomial time function can be represented by a proof of these logics via the proofs-as-programs correspondence. Furthermore, there is a reduction strategy which normalizes a given proof in polynomial time. Given the latter polynomial time “weak” normalization theorem, it is natural to ask whether a “strong” form of polynomial time normalization theorem holds or not. In this paper, we introduce an untyped term calculus, called Light Affine Lambda Calculus (λ{LA}), which corresponds to {ILAL}. λ{LA} is a bi-modal λ-calculus with certain constraints, endowed with very simple reduction rules. The main property of {LALC} is the polynomial time strong normalization: any reduction strategy normalizes a given λ{LA} term in a polynomial number of reduction steps, and indeed in polynomial time. Since proofs of {ILAL} are structurally representable by terms of λ{LA}, we conclude that the same holds for {ILAL}.},
	pages = {253--280},
	number = {3},
	journaltitle = {Archive for Mathematical Logic},
	shortjournal = {Arch. Math. Logic},
	author = {Terui, Kazushige},
	date = {2007},
	langid = {english},
	keywords = {Lambda calculus, Light logics, Polynomial time}
}

@article{plotkin_lcf_1977,
	title = {{LCF} considered as a programming language},
	volume = {5},
	abstract = {The paper studies connections between denotational and operational semantics for a simple programming language based on {LCF}. It begins with the connection between the behaviour of a program and its denotation. It turns out that a program denotes ⊥ in any of several possible semantics if it does not terminate. From this it follows that if two terms have the same denotation in one of these semantics, they have the same behaviour in all contexts. The converse fails for all the semantics. If, however, the language is extended to allow certain parallel facilities behavioural equivalence does coincide with denotational equivalence in one of the semantics considered, which may therefore be called “fully abstract”. Next a connection is given which actually determines the semantics up to isomorphism from the behaviour alone. Conversely, by allowing further parallel facilities, every r.e. element of the fully abstract semantics becomes definable, thus characterising the programming language, up to interdefinability, from the set of r.e. elements of the domains of the semantics.},
	pages = {223--255},
	number = {3},
	journaltitle = {Theoretical Computer Science},
	shortjournal = {Theoretical Computer Science},
	author = {Plotkin, Gordon D.},
	date = {1977},
	file = {ScienceDirect Full Text PDF:/home/gabriele/.zotero/zotero/fuzoq98e.default/zotero/storage/224ERIUT/Plotkin - 1977 - LCF considered as a programming language.pdf:application/pdf;ScienceDirect Snapshot:/home/gabriele/.zotero/zotero/fuzoq98e.default/zotero/storage/6LXYTWLU/0304397577900445.html:text/html}
}

@article{von_neumann_zur_1928,
	title = {Zur Theorie der Gesellschaftsspiele},
	volume = {100},
	pages = {295--320},
	number = {1},
	journaltitle = {Mathematische Annalen},
	shortjournal = {Math. Ann.},
	author = {von Neumann, J.},
	date = {1928},
	langid = {german}
}

@article{nash_equilibrium_1950,
	title = {Equilibrium points in n-person games},
	volume = {36},
	abstract = {One may define a concept of an n -person game in which each player has a finite set of pure strategies and in which a definite set of payments to the n players corresponds to each n -tuple of pure strategies, one strategy being taken for each player. For mixed strategies, which are probability distributions over the pure strategies, the pay-off functions are the expectations of the players, thus becoming polylinear forms …},
	pages = {48--49},
	number = {1},
	journaltitle = {Proceedings of the National Academy of Sciences},
	shortjournal = {{PNAS}},
	author = {Nash, John F.},
	date = {1950},
	langid = {english},
	file = {Full Text PDF:/home/gabriele/.zotero/zotero/fuzoq98e.default/zotero/storage/BHH9AZ7L/Nash - 1950 - Equilibrium points in n-person games.pdf:application/pdf;Snapshot:/home/gabriele/.zotero/zotero/fuzoq98e.default/zotero/storage/92A9DL5F/48.html:text/html}
}

@article{church_properties_1936,
	title = {Some properties of conversion},
	volume = {39},
	abstract = {Our mission is to further the interests of mathematical research, scholarship and education.},
	pages = {472--482},
	number = {3},
	journaltitle = {Transactions of the American Mathematical Society},
	shortjournal = {Trans. Amer. Math. Soc.},
	author = {Church, Alonzo and Rosser, J. B.},
	date = {1936},
	langid = {american},
	file = {Full Text PDF:/home/gabriele/.zotero/zotero/fuzoq98e.default/zotero/storage/JRGITXHN/Church e Rosser - 1936 - Some properties of conversion.pdf:application/pdf;Snapshot:/home/gabriele/.zotero/zotero/fuzoq98e.default/zotero/storage/VL5JX27U/home.html:text/html}
}

@article{barendregt_notes_1976,
	title = {Some notes on lambda-reduction},
	volume = {22},
	pages = {13--53},
	journaltitle = {Preprint of the University of Utrecht},
	author = {Barendregt, Hendrik Pieter and Bergstra, Jan and Klop, jan willem and Volken, Henri},
	date = {1976},
	file = {Full Text PDF:/home/gabriele/.zotero/zotero/fuzoq98e.default/zotero/storage/B8ZZMCKI/Barendregt et al. - 1976 - Some notes on lambda-reduction.pdf:application/pdf}
}

@article{church_set_1933,
	title = {A Set of Postulates For the Foundation of Logic},
	volume = {34},
	pages = {839--864},
	number = {4},
	journaltitle = {Annals of Mathematics},
	author = {Church, Alonzo},
	date = {1933}
}

@incollection{cardone_lambda-calculus_2009,
	title = {Lambda-Calculus and Combinators in the 20th Century},
	volume = {5},
	pages = {723--817},
	booktitle = {Handbook of the History of Logic},
	publisher = {North-Holland},
	author = {Cardone, Felice and Hindley, J. Roger},
	editor = {Gabbay, Dov M. and Woods, John},
	date = {2009},
	file = {ScienceDirect Snapshot:/home/gabriele/.zotero/zotero/fuzoq98e.default/zotero/storage/TC2GAHX5/S1874585709700184.html:text/html}
}

@article{takahashi_parallel_1995,
	title = {Parallel Reductions in {$\lambda$}-Calculus},
	volume = {118},
	abstract = {The notion of parallel reduction is extracted from the simple proof of the Church-Rosser theorem by Tait and Martin-L�f. Intuitively, this means to reduce a number of redexes (existing in a �-term) simultaneously. Thus in the case of �-reduction the effect of a parallel reduction is same as that of a "complete development" which is defined by using "residuals" of �-redexes. A nice feature of parallel reduction, however, is that it can be defined directly by induction on the structure of �-terms (without referring to residuals or other auxiliary notions), and the inductive definition provides us exactly what we need in proving the theorem inductively. Moreover, the notion can be easily extended to other reduction systems such as Girard�s second-order system F and G�del�s system T. In this paper, after reevaluating the significance of the notion of parallel reduction in Tait-and-Martin-L�f type proofs of the Church-Rosser theorems, we show that the notion of parallel reduction is also useful in giving short and direct proofs of some other fundamental theorems in reduction theory of �-calculus; among others, we give such simple proofs of the standardization theorem for �-reduction (a special case of which is known as the leftmost reduction theorem for �-reduction), the quasi-leftmost reduction theorem for �-reduction, the postponement theorem of �-reduction (in ߿-reduction), and the leftmost reduction theorem for ߿-reduction.v{\textgreater}},
	pages = {120--127},
	number = {1},
	journaltitle = {Inf. Comput.},
	author = {Takahashi, M.},
	date = {1995}
}

@article{huet_residual_1994,
	title = {Residual theory in {$\lambda$}-calculus: a formal development*},
	volume = {4},
	shorttitle = {Residual theory in λ-calculus},
	abstract = {We present the complete development, in Gallina, of the residual theory of β-reduction in pure λ-calculus. The main result is the Prism Theorem, and its corollary Lévy's Cube Lemma, a strong form of the parallel-moves lemma, itself a key step towards the confluence theorem and its usual corollaries (Church-Rosser, uniqueness of normal forms). Gallina is the specification language of the Coq Proof Assistant (Dowek et al., 1991; Huet 1992b). It is a specific concrete syntax for its abstract framework, the Calculus of Inductive Constructions (Paulin-Mohring, 1993). It may be thought of as a smooth mixture of higher-order predicate calculus with recursive definitions, inductively defined data types and inductive predicate definitions reminiscent of logic programming. The development presented here was fully checked in the current distribution version Coq V5.8. We just state the lemmas in the order in which they are proved, omitting the proof justifications. The full transcript is available as a standard library in the distribution of Coq.},
	pages = {371--394},
	number = {3},
	journaltitle = {Journal of Functional Programming},
	author = {Huet, Gérard},
	date = {1994},
	langid = {english},
	file = {Snapshot:/home/gabriele/.zotero/zotero/fuzoq98e.default/zotero/storage/34LMIINU/10C9E95ABFCEEFD4F1CBAF2C800647AA.html:text/html}
}

@article{shankar_mechanical_1988,
	title = {A Mechanical Proof of the Church-Rosser Theorem},
	volume = {35},
	abstract = {The Church-Rosser theorem is a celebrated metamathematical result on the lambda calculus. We describe a formalization and proof of the Church-Rosser theorem that was carried out with the Boyer-Moore theorem prover. The proof presented in this paper is based on that of Tait and Martin-Löf. The mechanical proof illustrates the effective use of the Boyer-Moore theorem prover in proof checking difficult metamathematical proofs.},
	pages = {475--522},
	number = {3},
	journaltitle = {J. {ACM}},
	author = {Shankar, N.},
	date = {1988}
}

@software{vanoni_prostra-tool_nodate,
	title = {prostra-tool},
	url = {https://github.com/Supervenom/prostra-tool},
	author = {Vanoni, Gabriele}
}

@book{hrbacek_introduction_1999,
	edition = {3rd edition},
	title = {Introduction to Set Theory},
	abstract = {Thoroughly revised, updated, expanded, and reorganized to serve as a primary text for mathematics courses, Introduction to Set Theory, Third Edition covers the basics: relations, functions, orderings, finite, countable, and uncountable sets, and cardinal and ordinal numbers. It also provides five additional self-contained chapters, consolidates the material on real numbers into a single updated chapter affording flexibility in course design, supplies end-of-section problems, with hints, of varying degrees of difficulty, includes new material on normal forms and Goodstein sequences, and adds important recent ideas including filters, ultrafilters, closed unbounded and stationary sets, and partitions.},
	publisher = {Marcel Dekker},
	author = {Hrbacek, Karel and Jech, Thomas},
	date = {1999}
}
