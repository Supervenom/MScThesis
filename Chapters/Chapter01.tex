%************************************************
\chapter{Introduction}\label{ch:introduction}
%************************************************
Functional programming languages are an alternative to the classical imperative paradigm. While imperative programs act on the state of the machine, functional ones are stateless, without any side effect. Functions, indeed, are intended as in mathematics. Unlike procedures, a function called with the same argument always returns the same result, i.e. referential transparency holds. This way, formal reasoning about programs, for example operated by compilers, is easier, since the context does not need to be considered \cite{backus_can_1978}. Moreover functional programs are typically more concise than their imperative counterpart. Typical features of functional programming languages, such as higher-order functions, algebraic data types and pattern matching, in fact ease the process of writing generic and abstract code, thus allowing modularity and code reuse. Then, why are they used so little? One of the reasons is certainly performances. The model of computation underlying functional programs is in fact very different from the hardware devoted to their execution and this gap can lead to a massive performance loss. This way, compilers become crucial in the process of software development, since depending on the type of translation they operate, the resulting machine code can be very different. In particular, in pure functional programs, different orders of evaluation of functions lead to the same result, because of the absence of side effects. However this order is fixed by compilers which operate a choice fundamental with respect to performances. Studying these phenomena directly on real programming languages and compilers is very hard. Indeed, one needs to consider some abstract models of computation. Since functional programming languages are the target, it seems natural to study their mechanisms in the context of the $\lambda$-calculus. The $\lambda$-calculus in fact is a very good model for functional languages, and in particular it allows a fine analysis of function calling mechanisms \cite{plotkin_call-by-name_1975}. Moreover, $\lambda$-calculus itself can be seen as an instance of an \emph{abstract reduction system}, a wider class of models used to study \emph{rewriting systems} in the abstract. This way, a lot of results in the abstract theory of rewriting can be directly applied to the $\lambda$-calculus. In this thesis we introduce for the first time the notion of randomised strategy in the scope of the $\lambda$-calculus. We give an informal motivating example which justifies our interest.
\begin{example}
	Let us consider the situation depicted in Figure~\ref{figure:intro}. In the initial state we are in $S$ and we want to reach $A$ in the minimum number of steps. We have to face a choice, going left or going right. Let us consider that we do not know anything about this maze and that our decisions are \emph{local}, i.e. not based on previous ones, and uniform, i.e. we should behave the same way at each fork e.g. always left. This way we have only two deterministic strategies: going always left and going always right. In both cases we take three steps to reach $A$. If we relax the uniformity constraint instead we can take two steps if we go first left and then right. Can we improve enhance our result \emph{maintaining} the uniformity constraint? What if at every fork we flip a coin and if the result is head we go left and otherwise we go right? Let us consider an unfair coin which has probability $p$, $0\leq p\leq 1$, of showing up head. The process of reaching $A$ from $S$ is now stochastic and thus the number of steps taken is a random variable, call it $X$. It is sensible to compute its expected value, as a function of $p$. We have to consider all possible paths from $S$ to $A$ and their respective probabilities. From standard probability theory we have:
	\begin{align*}
	\mathbb{E}_p[X]&=\sum\limits_{i=0}^{\infty} i\cdot\mathbb{P}\{X=i\}=2(p(1-p)+ (1-p)p)+3(p^2+(1-p)^2)\\
	&=2p^2-2p+3.
	\end{align*}
	First, we can note that in the case in which $p=0$ or $p=1$, i.e. in the cases in which the process is deterministic, $\mathbb{E}_p[X]=3$, as expected. Then, we note that $\mathbb{E}_p[X]$, seen as a function of $p$ is convex. In particular it has minimum in $p=0.5$ which corresponds to an average of $2.5$ steps. Hence, it is worth randomising.
\end{example}
\begin{figure}
	\fbox{
		\begin{minipage}{.96\textwidth}
	\centering
	\begin{tikzpicture}
		[node distance=20mm, auto, transform shape]
		\node (S) at (0,0) {$S$};
		\node (n) at (-1.5,-1.5) {$\bullet$};
		\node (l) at (1.5,-1.5) {$\bullet$};
		\node (r) at (-3,-3) {$\bullet$};
		\node (s) at (3,-3) {$\bullet$};
		\node (A) at (0,-6) {$A$};
		\draw (S) edge[->] node[left=5pt] {} (n);
		\draw (S) edge[->] node[right=5pt] {} (l);
		\draw (n) edge[->] node[left=5pt] {} (A);
		\draw (l) edge[->] node[right=5pt] {} (A);
		\draw (n) edge[->] node[right=5pt] {} (r);
		\draw (l) edge[->] node[right=5pt] {} (s);
		\draw (r) edge[->] node[right=5pt] {} (A);
		\draw (s) edge[->] node[right=5pt] {} (A);
	\end{tikzpicture}
	\end{minipage}}
	\caption{Introductory example}
	\label{figure:intro}
\end{figure}
The simple example above showed that randomising a choice could improve our utility, in that case decreasing the (expected) number of steps needed to reach $A$ from $S$. This is not a new result. In 1928 John von Neumann published his Minimax Theorem \cite{von_neumann_zur_1928}, which guarantees a rational outcome for every zero-sum game in mixed strategies, i.e. probability distributions on strategies. We are used, indeed, to play rock-scissor-paper with a randomised strategy that assigns equal probabilities to each of three possible moves. Moreover, John Nash proved the existence of Nash Equilibria in mixed strategies for the entire class of finite strategic games \cite{nash_equilibrium_1950}. However, we are not interested in games, since our optimisation problem is single agent. In fact, we want to minimise the number of steps to normal form for a given $\lambda$-term. This quantity is not fixed because in the $\lambda$-calculus there is some nondeterminism. As in numerical expressions, one can reduce a term in different ways. A property called confluence assures that the result, i.e. the normal form, is unique (if it exists), but the number of steps taken to reach it can vary. Informally, a reduction strategy gives a rule which specifies how to reduce any term, solving this nondeterminism. Of course, we are interested in those strategies which minimise the number of steps of any term to normal form. Unfortunately these \emph{minimal} strategies do not exist, or, better, they are not computable \cite{barendregt_lambda_1984}. Hence one should look for suboptimal ones. Reduction strategies for
the $\lambda$-calculus are typically defined according to the position
of the contracted \emph{redex} e.g. \emph{leftmost-outermost} ($\pslo$),
\emph{leftmost-innermost}, \emph{rightmost-innermost} ($\psri$). They have been studied a lot, since they are very linked to the study of evaluation orders in programming languages.
In general, innermost strategies are considered more efficient, because
programs often need to use their arguments more than once. However, rightmost-innermost is not normalising: there
exist terms which have a normal form which, however, can be missed by
innermost strategies. Instead, a classical result by Curry and Feys
\cite{curry_combinatory_1958}, states that the leftmost-outermost strategy is
\emph{normalising}, i.e., it always rewrites terms to their normal
norm, if it exists. Thus, leftmost-outermost is slower, but
safer. Could we get, in a sense, the best of both worlds?  All
reduction strategies for the $\lambda$-calculus in the literature up
to now are \emph{deterministic}, i.e. they are (partial) functions on
(possibly shared representations of) terms. What would
happen if the redexes to reduce were picked according to some
probability distribution? How many steps would a term need to reach a
normal form \emph{on the average}?
\section{Main Contributions and Results}
\begin{itemize}
\item We devised a randomised strategy called $\mathsf{P}_\varepsilon$. According to this strategy a term is reduced leftmost with probability $\varepsilon$ and rightmost with probability $1-\varepsilon$. We proved that $\mathsf{P}_\varepsilon$ is positive almost-surely normalising on
weakly normalising terms. That means that if a term $\termone$ has normal form, then the expected number of reduction steps from
$\termone$ to its normal form with strategy $\mathsf{P}_\varepsilon$
is finite. This is in contrast to the rightmost-innermost strategy. Rightmost-innermost, in other
words, is the only non normalising strategy in the family
$\{\mathsf{P}_\varepsilon\}_{0\leq\varepsilon\leq 1}$, namely $\mathsf{P}_0$. Also the uniform strategy, which reduces every redex with the same probability, does not satisfy this property. To formally define the notion of randomised strategy, we set up a mathematical framework, in the tradition of abstract rewriting theory, that we called FPARS, \emph{fully probabilistic abstract reduction system}.
\item We prove that there exists a class of terms and
$0<\mu<1$ for which $\mathsf{P}_\mu$ outperforms, on average, both
$\pslo$ and $\psri$. This shows that randomisation can
indeed be useful in this context. Thus, our strategy guarantees termination in finite expected time and good performances on some terms. This means that is non trivial and worth studying. In particular we try to capture some of the characteristics a term should have to benefit from this kind of randomisation.
\item We prove that the expected number of reduction steps to
normal form with strategy $\mathsf{P}_\varepsilon$, seen as a function
on $\varepsilon$, has minimum in $0$ (respectively, at $1$) for terms
in the sub-calculus $\lambda I$, i.e. where erasing arguments is forbidden (respectively, in $\lambda A$, i.e. where copying arguments is forbidden). This result actually is about the optimality of two deterministic strategies, namely rightmost-innermost and leftmost-outermost. However, we did not find any proof of this fact in the literature.
\end{itemize}
\section{Structure of the Work}
\paragraph{Chapter 2} After a brief discussion on traditional abstract reduction systems, we introduce our notion of FPARS. We define some important properties and prove a sufficient condition for positively almost-sure termination. Then we put FPARSs and their properties in relation with traditional Markov chain theory. In the end we suggest a taxonomy of different rewriting systems present in the literature.
\paragraph{Chapter 3} This chapter is devoted to the (meta-)theory of the $\lambda$-calculus. After a brief overview, the syntax is presented. Then, two classical results are discussed. More space is given to discuss strategies and their properties. In this section are present standard results along with some original lemmmata used in the proofs of main results of Chapter~\ref{chapter:prostra}.
\paragraph{Chapter 4} Randomised strategies for the $\lambda$-calculus are defined and discussed in detail. Main results about $\mathsf{P}_\varepsilon$, such as positive almost-sure termination, are proved.
\paragraph{Chapter 5} We present the automatic tool used to investigate strategy $\mathsf{P}_\varepsilon$ and some further experimental results obtained through the use of such tool. Then, we discuss some possible future directions of research about randomised strategies.
%*****************************************
