%************************************************
\chapter{Further Investigations and Future Work}
%************************************************
We have already asserted that studying the randomised strategy $\mathsf{P}_\varepsilon$ in the scope of the full $\lambda$-calculus is hard. More precisely the following problems are conjectured to be very hard.
\begin{itemize}
	\item Finding a class of terms $\mathcal{C}\subset\Lambda$ such that for each term $\termone\in\mathcal{C}$, there exists $0<\varepsilon<1$ such that $\explen{\termone}(\varepsilon)<\nsteps{\pslo}(\termone)$ and $\explen{\termone}(\varepsilon)<\nsteps{\psri}(\termone)$.
	\item Computing in closed form, for each term $\termone\in\mathcal{C}$, $\explen{\termone}(\termone)$.
\end{itemize}
Thus our investigations should lose some generality. In the previous Chapter we have restricted the set of terms, considering the sub-calculi $\lambda A$ and $\lambda I$. In this Chapter, instead, we use an experimental approach trying to find a class $\mathcal{C}$ as above. In order to check if $\termone\in\mathcal{C}$, we proceed computing $\explen{\termone}(\epsilon)$ and then testing if the minimum $\mu$ is such that $0<\mu<1$. How can we determine $\explen{\termone}(\epsilon)$? Since the problem is undecidable we provide a pseudo-algorithm, that intuitively builds trees like those we have presented in the previous Chapter. Because of the presence of infinite reduction paths, some heuristics has to be adopted in order to have a finite representation. We restricted to a particular, yet meaningful case: terms that rewrite to themselves. This way, during the construction of the tree, when a term rewrites to itself, a self-loop is added and no child is added. This method clearly is not complete. For example we would not be able to represent finitely terms like $\termone = (\lambda x.\lambda y.y)\bm{\Delta_4}$. Instead we are able to tackle the $\bm{\Omega}$ combinator.
\section{The Tool}
To test a large number of terms, the use of pencil and paper would not be feasible. Thus, we developed an automatic tool performing all the computations \cite{vanoni_prostra-tool_nodate}. The tool is composed of three different components.
\begin{itemize}
	\item An interpreter of the $\lambda$-calculus with both $\pslo$ and $\psri$ strategies. This component implements a parser, $\alpha$-conversion and the actual evaluator implementing $\beta$-reduction.
	\item A component implementing the pseudo-algorithm building the tree from a term $\termone$ and then computing $\explen{\termone}(\varepsilon)$.
	\item An analyser of $\explen{\termone}(\varepsilon)$, that plots it and computes its derivative and its minimum. 
\end{itemize}
There is a conceptual difference between the first two components, dealing mainly with symbolic manipulation of terms and structures, and the last one dealing with computer algebra, numerical analysis and data visualisation. For this reason first two components were implemented in \texttt{Haskell} while the last one in \texttt{Python}. In fact, \texttt{Haskell} is perfect to manage inductive data structures such as $\lambda$-terms while \texttt{Python} provides a library, \textsf{Scipy}, comprehending libraries for computer algebra (\textsf{SymPy}), numerical analysis (\textsf{SciPy} and \textsf{NumPy}) and data visualisation (\textsf{Matplotlib}). Communication between the two languages is achieved through a common syntax for symbolic expressions.
\subsection{Main Data Types and Functions}
According to the grammar defining the $\lambda$-calculus, terms are defined as:
$$
\texttt{data Term = Var String | Abs Name Term | App Term Term}
$$
Substitution is easily defined thanks to pattern matching:
\begin{align*}
&\texttt{subst :: Term -> String -> Term -> Term}\\
&\texttt{subst (Var x) y z}\\
&\qquad\texttt{|x == y = z}\\
&\qquad\texttt{|otherwise = (Var x)}\\
&\texttt{subst (Abs y x) z w = (Abs y (subst x z w))}\\
&\texttt{subst (App x y) z w = (App (subst x z w) (subst y z w))}
\end{align*}
Please note that this substitution is \emph{not} capture avoiding. Terms should be $\alpha$-converted first. $\beta$-reduction is defined as usual:
\begin{align*}
&\texttt{beta :: Term -> Term}\\
&\texttt{beta (App (Abs y x) z) = subst x y z}
\end{align*}
Since the reduction process depends on the reduction strategy, \texttt{reduce} is a higher-order function. A heuristics halts the reduction process when a term reduces to itself:
\begin{align*}
&\texttt{reduce :: (Term -> Term) -> Term -> Term}\\
&\texttt{reduce eval term}\\
&\qquad\texttt{|(eval term) == term = term}\\
&\qquad\texttt{|otherwise = reduce eval (eval term)}
\end{align*}
Leftmost-outermost strategy reduces always the leftmost redex, if any, and is implemented in the following way (rightmost-innermost is symmetrical):
\begin{align*}
&\texttt{evalLO :: Term -> Term}\\
&\texttt{evalLO (Var x) = Var x}\\
&\texttt{evalLO (Abs y x) = Abs y (evalLO x)}\\
&\texttt{evalLO (App x y)}\\
&\qquad\texttt{|(isRedex (App x y)) = beta (App x y)}\\
&\qquad\texttt{|(hasRedexes x) = (App (evalLO x) y)}\\
&\qquad\texttt{|(hasRedexes y) = (App x (evalLO y))}\\
&\qquad\texttt{|otherwise = (App x y)}
\end{align*}
The tree (eventually with self-loops) for each term is represented by the following data type:
\begin{align*}
	\texttt{data PTree = }&\texttt{PLeaf | NodeOne PTree |}\\
	&\texttt{NodeOneCycle PTree | NodeTwo PTree PTree}
\end{align*}
The tree is built simply considering the reduction of the term $\termone$ according to $\pslo$ and $\psri$ strategy. A \texttt{PLeaf} is created if the term is in normal form, a \texttt{NodeOne} if $\pslo$-reduct and $\psri$-reduct coincide, a \texttt{NodeTwo} if they are distinct and a \texttt{NodeCycle} if the $\psri$-reduct rewrites to itself.
\begin{align*}
&\texttt{buildPTree :: Term -> PTree}\\
&\texttt{buildPTree term}\\
&\qquad\texttt{|not (hasRedexes term) = PLeaf}\\
&\qquad\texttt{|(evalLO term) == (evalRI term)}\\
&\quad\qquad\texttt{= NodeOne (buildPTree (evalLO term))}\\
&\qquad\texttt{|(evalRI term) == term}\\
&\qquad\quad\texttt{= NodeOneCycle (buildPTree (evalLO term))}\\
&\qquad\texttt{|otherwise = NodeTwo (buildPTree (evalLO term))}\\
&\qquad\quad\texttt{(buildPTree (evalRI term))}
\end{align*}
Once one has built the tree for a term $\termone$, it is easy to compute $\explen{\termone}$. If $\termone\redlo\termtwo$ and $\termone\redri\termthree$, then
$$
\explen{\termone}=1+\varepsilon\cdot\explen{\termtwo}+(1-\varepsilon)\cdot\explen{\termthree}.
$$
We have already proved in Example \ref{example:omega} that if $\termone\redri\termone$ and $\explen{\termtwo}(\varepsilon)=0$, then $\explen{\termone}(\varepsilon)=\frac{1}{\varepsilon}$. The construction can be gereralised to arbitrary values of $\explen{\termtwo}$, yielding $\explen{\termone}(\varepsilon)=\frac{1}{\varepsilon}+\explen{\termtwo}$. Clearly if $\termone$ is in normal form, then $\explen{\termone}=0$. All this ingredients allow to compute $\explen{\termone}$ inductively on the structure of the tree. This is a pseudo-algorithm, in that it is not assured to terminate. Diverging terms that do not rewrite to themselves in fact yield to non-terminating computations. We report the code in \texttt{pseudo-Haskell}.
\begin{align*}
&\texttt{bExpr :: PTree -> Expression}\\
&\texttt{bExpr PLeaf = 0}\\
&\texttt{bExpr (NodeOne t) = 1 + (bExpr t)}\\
&\texttt{bExpr (NodeOneCycle t) = 1/}\varepsilon\texttt{ + (bExpr t)}\\
&\texttt{bExpr (NodeTwo t1 t2) = 1 + }\varepsilon\texttt{*(bExpr t1) + (1-}\varepsilon\texttt{)*(bExpr t2)}
\end{align*}
\section{Experimental Results}
Once the tool is set, it is easy to test different terms. Our goal is to try to find a subset of the class $\mathcal{C}$ of terms for which strategy $\mathsf{P}_\varepsilon$, $\varepsilon\not\in\{0,1\}$, is more efficient than $\pslo$ and $\psri$. We already know that $\mathcal{C}\subseteq\Lambda\setminus(\Lambda_A\cup\Lambda_I)$. First we give an answer to the following trivial question: is $\mathcal{C}$ a proper subset of $\Lambda\setminus(\Lambda_A\cup\Lambda_I)$?
\begin{proposition}\label{prop:strict}
	$\mathcal{C}\subset\Lambda\setminus(\Lambda_A\cup\Lambda_I)$.
\end{proposition}
\begin{proof}
	We provide a family of terms $\termone_n\in\Lambda\setminus(\Lambda_A\cup\Lambda_I)$ such that $\nsteps{\pslo}(\termone)=\explen{\termone}(1)<\explen{\termone}(\varepsilon)$ if $0<\varepsilon<1$.
	$$
	\termone_n=\termthree_n((\lambda y.z)\bm{\Omega})
	$$
	where:
	$$
	\termthree_n = C_n((\lambda x.x)y)\qquad
	C_n = \lambda  x.\underbrace{xx\cdots x}_{n\text{ times}}
	$$
	It is then easy to compute the average number of steps to normal form that is
	$$
	\explen{\termone_n}(\varepsilon)=\frac{n+2}{\varepsilon}.
	$$ 
	The function $\explen{\termone_n}$ is strictly decreasing.\\
	This proves the claim.
\end{proof}
We have at this point examples of terms in and out of $\mathcal{C}$. What differences there are between them? Can they be spotted \emph{algorithmically} with \emph{low} complexity? Answering this question seems really hard. Hence we decided for the moment to limit ourselves in finding other terms that behave well w.r.t. strategy $\mathsf{P}_\varepsilon$. A starting point in our investigation is term $\termone_n$ from Example~\ref{example:nontr}. We look for other terms changing little by little its structure. Thanks to the automatic tool, we have an immediate feedback, so that we can try many different alternatives. Term $\termone_n$ is not strongly normalising. Is that a necessary condition? What happens if we replace the combinator $\bm{\Omega}$ in $\termone_n$ (actually in the $\pslo$-reduct of $\termone_n$), with $\mathbf{I}^m$, i.e. the identity applied to itself $m$ times? We call this term $\termtwo_{n,m}=((\lambda y.z)\bm{I}^m) (C_n(\bm{I}y))$. In Figure~\ref{figure:strnor} $\explen{\termtwo_{4,4}}(\varepsilon)$ is plotted. Also strongly normalising terms can, indeed, take advantage from randomised strategies. Moreover, this case is particularly interesting because there is a sort of symmetry. This leads to $\nsteps{\pslo}=\nsteps{\psri}=6$.
\begin{figure}
	\fbox{
		\begin{minipage}{.96\textwidth}
			\centering
			\scalebox{1}{\begin{adjustbox}{clip,trim=2.5in 2in 0.5in 0.4in}\input{gfx/plot8.pgf}\end{adjustbox}}
			\vspace{5pt}
		\end{minipage}}
		\caption{The function $\explen{\termtwo_{4,4}}(\varepsilon)$.}
		\label{figure:strnor}
	\end{figure}
What if we move $C_n$ inside? This way we have a term of this form: $\termthree_n=((\lambda y.C_n)\bm{\Omega})(\bm{I}y)$. $\explen{\termthree_9}(\varepsilon)$ is reported in Figure~\ref{figure:plot2}.
\begin{figure}
	\fbox{
		\begin{minipage}{.96\textwidth}
			\centering
			\scalebox{1}{\begin{adjustbox}{clip,trim=2.5in 2in 0.5in 0.4in}\input{gfx/plot4.pgf}\end{adjustbox}}
			\vspace{5pt}
		\end{minipage}}
		\caption{The function $\explen{\termthree_9}(\varepsilon)$.}
		\label{figure:plot2}
	\end{figure}
What about the position of the minimum point? We can observe that the optimal $\varepsilon$ decreases when $n$ increases. In fact, as expected, more and more one has to copy the argument, more it is convenient to reduce it first (Figure~\ref{figure:plot3}).
\begin{figure}
	\fbox{
		\begin{minipage}{.96\textwidth}
			\centering
			\scalebox{1}{\begin{adjustbox}{clip,trim=2.5in 2in 0.5in 0.4in}\input{gfx/plot7.pgf}\end{adjustbox}}
			\vspace{5pt}
		\end{minipage}}
		\caption[The function $\explen{\termthree_3}(\varepsilon)$.]{The function $\explen{\termthree_3}(\varepsilon)$, in green, has minimum in \texttt{0.56}. $\explen{\termthree_9}(\varepsilon)$, in blue, has minimum in \texttt{0.38}. $\explen{\termthree_{16}}(\varepsilon)$, in red, has minimum in \texttt{0.31}.}
		\label{figure:plot3}
	\end{figure}
\section{Future Work}
This thesis is the first work on randomised strategies in the $\lambda$-calculus appeared in the literature. Thus, we could only scratch the surface. A lot of interesting problems remain open and we plan to investigate more in next months. In particular different questions have been raised.
\paragraph{Are there better randomised strategies?} After a brief account on the uniform strategy, that we classified as uninteresting since it is not PAST, we have introduced strategy $\mathsf{P}_\varepsilon$. It has been our only object of study throughout the whole work. However, one can think about other interesting strategies. For example, one could vary $\mathsf{P}_\varepsilon$ in a way such that probability $1-\varepsilon$ assigned to the $\psri$-redex is split uniformly between all redexes that have their argument in normal form. This way, the strategy looks more natural and less arbitrary. What about its performances?
\paragraph{How to tune $\varepsilon$?} Tuning parameter $\varepsilon$ is essential to get good performances. How can we choose its value, given a generic term $\termone$? Looking for the value that minimises $\explen{\termone}$ seems infeasible, but also finding suboptimal values seems hard. In particular the algorithm looking for the value should not consume more time than the gain in performances obtained through the use of the randomised strategy.
\paragraph{What about the real world?} Our target language was the $\lambda$-calculus. However no one programs in that language. Even restricting to functional programming languages, would our technique be useful in real world interpreters and compilers? Would real world programs benefit from our strategy? An intermediate step could be considering a simple \emph{typed} language such as \texttt{PCF} \cite{plotkin_lcf_1977}. 